{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "mnist = input_data.read_data_sets('MNIST_data/',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps=10000\n",
    "batch_size=128\n",
    "learning_rate = 0.0002\n",
    "# G , D - DNN (256개)\n",
    "image_dim = 784\n",
    "gen_hidden_dim = 256\n",
    "disc_hidden_dim = 256\n",
    "noise_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사비에르 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_init(shape):\n",
    "    return tf.random_normal(shape=shape, stddev=1. / tf.sqrt(shape[0] / 2.)) \n",
    "\n",
    "weights = {\n",
    "    'gen_hidden1': tf.Variable(he_init([noise_dim, gen_hidden_dim])),\n",
    "    'gen_out' : tf.Variable(he_init([gen_hidden_dim, image_dim])),\n",
    "    'disc_hidden1' : tf.Variable(he_init([image_dim, disc_hidden_dim])),\n",
    "    'disc_out' : tf.Variable(he_init([disc_hidden_dim,1]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'gen_hidden1' : tf.Variable(tf.zeros([gen_hidden_dim])),\n",
    "    'gen_out': tf.Variable(tf.zeros([image_dim])),\n",
    "    'disc_hidden1' : tf.Variable(tf.zeros([disc_hidden_dim])),\n",
    "    'disc_out' : tf.Variable(tf.zeros([1]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(x):\n",
    "    hidden_layer = tf.matmul(x, weights['gen_hidden1'])\n",
    "    hidden_layer = tf.add(hidden_layer, biases['gen_hidden1'])\n",
    "    hidden_layer = tf.nn.relu(hidden_layer)\n",
    "    output_layer = tf.matmul(hidden_layer, weights['gen_out'])\n",
    "    output_layer = tf.add(output_layer, biases['gen_out'])\n",
    "    output_layer = tf.nn.sigmoid(output_layer)\n",
    "    return output_layer\n",
    "\n",
    "def discriminator(x):\n",
    "    hidden_layer = tf.matmul(x, weights['disc_hidden1'])\n",
    "    hidden_layer = tf.add(hidden_layer, biases['disc_hidden1'])\n",
    "    hidden_layer = tf.nn.relu(hidden_layer)\n",
    "    output_layer = tf.matmul(hidden_layer, weights['disc_out'])\n",
    "    output_layer = tf.add(output_layer, biases['disc_out'])\n",
    "    output_layer = tf.nn.sigmoid(output_layer)\n",
    "    return output_layer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_input = tf.placeholder(tf.float32, shape=[None, noise_dim], name='input_noise')\n",
    "disc_input = tf.placeholder(tf.float32, shape=[None, image_dim], name='disc_input')\n",
    "\n",
    "gen_sample = generator(gen_input)\n",
    "disc_real = discriminator(disc_input) # Real 데이터를 받는 애랑\n",
    "disc_fake = discriminator(gen_sample) # Facke 이미지를 받는 애 두개\n",
    "\n",
    "gen_loss = -tf.reduce_mean(tf.log(disc_fake))\n",
    "# 밑에 식이 log(D(x))+log(1-D(G(x)))\n",
    "disc_loss = -tf.reduce_mean(tf.log(disc_real) + tf.log(1.- disc_fake))\n",
    "\n",
    "optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "gen_vars = [weights['gen_hidden1'], weights['gen_out'],\n",
    "           biases['gen_hidden1'], biases['gen_out']]\n",
    "disc_vars = [weights['disc_hidden1'], weights['disc_out'],\n",
    "           biases['disc_hidden1'], biases['disc_out']]\n",
    "\n",
    "train_gen = optimizer_gen.minimize(gen_loss, var_list = gen_vars)\n",
    "train_disc = optimizer_disc.minimize(disc_loss, var_list = disc_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 G Loss: 0.28461903 D Loss: 1.9181436\n",
      "Step: 100 G Loss: 4.3973317 D Loss: 0.047020227\n",
      "Step: 200 G Loss: 3.4672449 D Loss: 0.14827183\n",
      "Step: 300 G Loss: 2.5136147 D Loss: 0.34276146\n",
      "Step: 400 G Loss: 2.8394322 D Loss: 0.24390507\n",
      "Step: 500 G Loss: 3.0885825 D Loss: 0.12815924\n",
      "Step: 600 G Loss: 3.2595859 D Loss: 0.13474798\n",
      "Step: 700 G Loss: 3.4005065 D Loss: 0.08104004\n",
      "Step: 800 G Loss: 3.5552144 D Loss: 0.078812055\n",
      "Step: 900 G Loss: 3.6366167 D Loss: 0.06815943\n",
      "Step: 1000 G Loss: 3.6196246 D Loss: 0.09625015\n",
      "Step: 1100 G Loss: 3.5145905 D Loss: 0.06541948\n",
      "Step: 1200 G Loss: 3.52983 D Loss: 0.0768328\n",
      "Step: 1300 G Loss: 3.7536216 D Loss: 0.09620807\n",
      "Step: 1400 G Loss: 3.868499 D Loss: 0.077326566\n",
      "Step: 1500 G Loss: 4.147256 D Loss: 0.0561004\n",
      "Step: 1600 G Loss: 4.3674355 D Loss: 0.051446926\n",
      "Step: 1700 G Loss: 4.011893 D Loss: 0.06920449\n",
      "Step: 1800 G Loss: 3.9665887 D Loss: 0.104530305\n",
      "Step: 1900 G Loss: 4.5099363 D Loss: 0.04063089\n",
      "Step: 2000 G Loss: 4.8489175 D Loss: 0.03483531\n",
      "Step: 2100 G Loss: 5.0848045 D Loss: 0.028644558\n",
      "Step: 2200 G Loss: 4.949232 D Loss: 0.04725016\n",
      "Step: 2300 G Loss: 5.0937185 D Loss: 0.04430207\n",
      "Step: 2400 G Loss: 5.585104 D Loss: 0.020710245\n",
      "Step: 2500 G Loss: 5.3402195 D Loss: 0.021660313\n",
      "Step: 2600 G Loss: 5.235427 D Loss: 0.017317947\n",
      "Step: 2700 G Loss: 5.8643365 D Loss: 0.01773665\n",
      "Step: 2800 G Loss: 5.563094 D Loss: 0.008682117\n",
      "Step: 2900 G Loss: 6.265473 D Loss: 0.008353142\n",
      "Step: 3000 G Loss: 5.815782 D Loss: 0.006291974\n",
      "Step: 3100 G Loss: 5.7340217 D Loss: 0.007872062\n",
      "Step: 3200 G Loss: 5.8879347 D Loss: 0.0118665295\n",
      "Step: 3300 G Loss: 5.7660513 D Loss: 0.0063683735\n",
      "Step: 3400 G Loss: 5.6296096 D Loss: 0.011624865\n",
      "Step: 3500 G Loss: 5.8993993 D Loss: 0.0105389645\n",
      "Step: 3600 G Loss: 4.997097 D Loss: 0.01789729\n",
      "Step: 3700 G Loss: 5.2281876 D Loss: 0.0153602585\n",
      "Step: 3800 G Loss: 3.701142 D Loss: 0.0698553\n",
      "Step: 3900 G Loss: 3.0108397 D Loss: 0.11268633\n",
      "Step: 4000 G Loss: 3.1508756 D Loss: 0.13596243\n",
      "Step: 4100 G Loss: 3.0787807 D Loss: 0.15317045\n",
      "Step: 4200 G Loss: 3.8997824 D Loss: 0.078955\n",
      "Step: 4300 G Loss: 4.814512 D Loss: 0.05110848\n",
      "Step: 4400 G Loss: 4.873617 D Loss: 0.038161606\n",
      "Step: 4500 G Loss: 4.408082 D Loss: 0.06114746\n",
      "Step: 4600 G Loss: 4.5837193 D Loss: 0.019311624\n",
      "Step: 4700 G Loss: 4.589284 D Loss: 0.05078762\n",
      "Step: 4800 G Loss: 4.740553 D Loss: 0.027373884\n",
      "Step: 4900 G Loss: 4.5498457 D Loss: 0.05231806\n",
      "Step: 5000 G Loss: 4.368589 D Loss: 0.04394591\n",
      "Step: 5100 G Loss: 4.4258556 D Loss: 0.03637871\n",
      "Step: 5200 G Loss: 5.0917654 D Loss: 0.020558387\n",
      "Step: 5300 G Loss: 4.9990606 D Loss: 0.08695042\n",
      "Step: 5400 G Loss: 4.863227 D Loss: 0.04058908\n",
      "Step: 5500 G Loss: 5.73055 D Loss: 0.061872903\n",
      "Step: 5600 G Loss: 5.4078083 D Loss: 0.04925244\n",
      "Step: 5700 G Loss: 4.87622 D Loss: 0.047253378\n",
      "Step: 5800 G Loss: 4.724191 D Loss: 0.087465785\n",
      "Step: 5900 G Loss: 4.7050757 D Loss: 0.06452648\n",
      "Step: 6000 G Loss: 4.310035 D Loss: 0.088192515\n",
      "Step: 6100 G Loss: 4.4848957 D Loss: 0.08378388\n",
      "Step: 6200 G Loss: 3.975007 D Loss: 0.09002462\n",
      "Step: 6300 G Loss: 4.422325 D Loss: 0.16911577\n",
      "Step: 6400 G Loss: 3.555417 D Loss: 0.22755079\n",
      "Step: 6500 G Loss: 4.3665037 D Loss: 0.09174108\n",
      "Step: 6600 G Loss: 3.976471 D Loss: 0.10461161\n",
      "Step: 6700 G Loss: 4.886085 D Loss: 0.10796739\n",
      "Step: 6800 G Loss: 3.4257927 D Loss: 0.14271644\n",
      "Step: 6900 G Loss: 4.1652207 D Loss: 0.11944312\n",
      "Step: 7000 G Loss: 3.6858573 D Loss: 0.13299061\n",
      "Step: 7100 G Loss: 3.237404 D Loss: 0.14668113\n",
      "Step: 7200 G Loss: 3.7116208 D Loss: 0.1376153\n",
      "Step: 7300 G Loss: 4.037626 D Loss: 0.1233463\n",
      "Step: 7400 G Loss: 3.8678074 D Loss: 0.09474018\n",
      "Step: 7500 G Loss: 3.864751 D Loss: 0.082475245\n",
      "Step: 7600 G Loss: 4.0967693 D Loss: 0.11681412\n",
      "Step: 7700 G Loss: 4.1654463 D Loss: 0.082094\n",
      "Step: 7800 G Loss: 4.287119 D Loss: 0.07693982\n",
      "Step: 7900 G Loss: 3.6461132 D Loss: 0.1470092\n",
      "Step: 8000 G Loss: 4.420921 D Loss: 0.08218595\n",
      "Step: 8100 G Loss: 4.4603634 D Loss: 0.09988549\n",
      "Step: 8200 G Loss: 3.9692075 D Loss: 0.09427892\n",
      "Step: 8300 G Loss: 3.7187603 D Loss: 0.15072383\n",
      "Step: 8400 G Loss: 3.959857 D Loss: 0.0918884\n",
      "Step: 8500 G Loss: 3.9177833 D Loss: 0.1518073\n",
      "Step: 8600 G Loss: 4.126211 D Loss: 0.19208673\n",
      "Step: 8700 G Loss: 4.3761086 D Loss: 0.12958376\n",
      "Step: 8800 G Loss: 4.347916 D Loss: 0.1847975\n",
      "Step: 8900 G Loss: 3.390686 D Loss: 0.1736705\n",
      "Step: 9000 G Loss: 4.656043 D Loss: 0.21162386\n",
      "Step: 9100 G Loss: 4.4840913 D Loss: 0.14231181\n",
      "Step: 9200 G Loss: 4.459738 D Loss: 0.09001534\n",
      "Step: 9300 G Loss: 3.7417998 D Loss: 0.11452862\n",
      "Step: 9400 G Loss: 4.402116 D Loss: 0.13955782\n",
      "Step: 9500 G Loss: 3.8883066 D Loss: 0.13744822\n",
      "Step: 9600 G Loss: 3.8204813 D Loss: 0.13159177\n",
      "Step: 9700 G Loss: 3.985334 D Loss: 0.080488846\n",
      "Step: 9800 G Loss: 4.0327654 D Loss: 0.092405215\n",
      "Step: 9900 G Loss: 4.6384335 D Loss: 0.0784389\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(num_steps):\n",
    "    batch_xs, _ = mnist.train.next_batch(batch_size) # disc의 인풋\n",
    "    z = np.random.uniform(-1.,1.,size=[batch_size, noise_dim]) # gen 인풋\n",
    "    _, _, gl, dl = sess.run([train_gen,train_disc,gen_loss, disc_loss],\n",
    "                        feed_dict={disc_input:batch_xs, gen_input:z})\n",
    "    if i % 100 == 0:\n",
    "        print(\"Step:\",i,\"G Loss:\",gl,\"D Loss:\",dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
