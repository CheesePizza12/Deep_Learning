{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mook\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "mnist = input_data.read_data_sets('MNIST_data/',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = mnist.train.images[:,:]\n",
    "y_train = mnist.train.labels[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = mnist.test.images[:,:]\n",
    "y_test = mnist.test.labels[:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN+DNN+softmax+xavier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프로젝트 CNN  Layer 구성하기 \n",
    "# Condition\n",
    "# 1층 : 3*3 필터 32개 (dropout rate: 0.3 )\n",
    "# relu-max pooling\n",
    "# 2층 : 3*3 필터 64개 (dropout rate: 0.3 )\n",
    "# relu-max pooling\n",
    "# 3층 : 3*3 필터 128개 (dropout rate: 0.3 )\n",
    "# relu-max pooling\n",
    "\n",
    "# DNN(FC)  Layer\n",
    "# Initializer : xavier initializer\n",
    "# 1층 : hidden node 625개 (dropout rate: 0.3 )\n",
    "# activation function : Relu\n",
    "# 2층 : hidden node 256개 (dropout rate: 0.3 )\n",
    "\n",
    "# Softmax Layer로 multi label classification\n",
    "# AdamOptimizer\n",
    "# learning_rate : 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_epoch = 15\n",
    "batch_size =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-b425b2749617>:47: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CNN에서 이미지를 전처리하고 DNN에서 구현한다 - CNN은 독립적으로 사용하지 않음\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# CNN에 이미지를 넣으면 data를 image로 바꿔야 한다\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])  \n",
    "W1 = tf.get_variable(\"W1\", shape=[3, 3, 1, 32], initializer=tf.contrib.layers.xavier_initializer())  # weight 변수 지정\n",
    "b1 = tf.Variable(tf.random_normal([32]))   # convol (filter) layer\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides = [1,1,1,1], padding = \"SAME\")\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")  # pooling\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)   # dropout = 30% 제외\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[3, 3, 32, 64], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([64]))\n",
    "L2 = tf.nn.conv2d(L1, W2, strides = [1,1,1,1], padding = \"SAME\")\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[3, 3, 64, 128], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([128]))\n",
    "L3 = tf.nn.conv2d(L2, W3, strides = [1,1,1,1], padding = \"SAME\")\n",
    "L3 = tf.nn.relu(L3)\n",
    "L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)   # dropout은 다음 층으로 넘어가기 직전에 실시\n",
    "\n",
    "L3_flat = tf.reshape(L3, [-1, 128*4*4])   # SAME = 28 / 2 = 14 / 2 = 7 / 2 = 4\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[128*4*4, 625], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "b4 = tf.Variable(tf.random_normal([625]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4) \n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[625, 256], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "b5 = tf.Variable(tf.random_normal([256]))\n",
    "L5 = tf.nn.relu(tf.matmul(L4, W5) + b5) \n",
    "L5 = tf.nn.dropout(L5, keep_prob=keep_prob)\n",
    "\n",
    "W6 = tf.get_variable(\"W6\", shape=[256, 10], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "b6 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "logits = tf.matmul(L5, W6) + b6\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epoch):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)  \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)  \n",
    "        c, _ = sess.run([cost, optimizer], feed_dict={X:batch_xs, Y:batch_ys})\n",
    "        avg_cost += c/total_batch \n",
    "    print(\"Epoch: \", epoch, \"Cost: \", avg_cost) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
