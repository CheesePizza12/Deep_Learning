{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "mnist = input_data.read_data_sets('MNIST_data/',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxScalar(data):\n",
    "    bunja = data-np.min(data,0)\n",
    "    bunmo = np.max(data,0) - np.min(data,0)\n",
    "    return bunja/(bunmo+1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 연습문제 워밍업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "              [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "              [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "              [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "              [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "              [819, 823, 1198100, 816, 820.450012],\n",
    "              [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "              [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mook\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-4-5190a6264e53>\", line 1, in <module>\n",
      "    x_data.shape\n",
      "NameError: name 'x_data' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mook\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mook\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\mook\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mook\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\mook\\Anaconda3\\lib\\inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\mook\\Anaconda3\\lib\\inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\mook\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\mook\\Anaconda3\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Users\\mook\\Anaconda3\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\mook\\Anaconda3\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\mook\\Anaconda3\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy= MinMaxScalar(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = xy[:,:-1]\n",
    "y_data = xy[:,[-1]]\n",
    "X= tf.placeholder(tf.float32, shape=[None,4])\n",
    "Y= tf.placeholder(tf.float32, shape=[None,1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4,1]))\n",
    "B = tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "hypothesis = tf.matmul(X,W)+B\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2000):\n",
    "    cost_v, _ = sess.run([cost, train], feed_dict={X:x_data,Y:y_data})\n",
    "    if step %100 ==0 :\n",
    "        print('step', step, 'cost',cost_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist.train.images.shape)\n",
    "print(mnist.train.labels.shape)\n",
    "print(mnist.train.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = mnist.train.images[:,:]\n",
    "y_train = mnist.train.labels[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = mnist.test.images[:,:]\n",
    "y_test = mnist.test.labels[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= tf.placeholder(tf.float32, shape=[None,28*28*1])\n",
    "Y= tf.placeholder(tf.float32, shape=[None,10])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([28*28*1,10]))\n",
    "B = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "logits = tf.matmul(X, W) + B\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "### accuracy\n",
    "prediction = tf.argmax(hypothesis,1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(1000):\n",
    "    cost_v, _ = sess.run([cost, optimizer], feed_dict={X:x_train,Y:y_train})\n",
    "    if step %100 ==0 :\n",
    "        print('step', step, 'cost',cost_v)\n",
    "print(sess.run(accuracy, feed_dict={X:x_train,Y:y_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sess.run(accuracy, feed_dict={X:x_test,Y:y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 나워서 하기 training_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epoch = 15\n",
    "batch_size = 100\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(training_epoch):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size) #550\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        c, _ = sess.run([cost, optimizer],feed_dict={X:batch_xs,Y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "    print(\"Epoch :\",epoch, 'Cost', avg_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sess.run(accuracy, feed_dict={X:x_test,Y:y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 아무거나 뽑아서 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = random.randint(0,mnist.test.num_examples-1)\n",
    "print('Lbael:', sess.run(tf.argmax(mnist.test.labels[r:r+1],1)))\n",
    "print('Lbael:', sess.run(tf.argmax(hypothesis,1),feed_dict={X:mnist.test.images[r:r+1]}))\n",
    "plt.imshow(\n",
    "mnist.test.images[r:r+1].reshape(28,28),\n",
    "    cmap='Greys',\n",
    "    interpolation = 'nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝으로 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist.train.images.shape)\n",
    "print(mnist.train.labels.shape)\n",
    "print(mnist.train.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = mnist.train.images[:,:]\n",
    "y_train = mnist.train.labels[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = mnist.test.images[:,:]\n",
    "y_test = mnist.test.labels[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= tf.placeholder(tf.float32, shape=[None,28*28*1])\n",
    "Y= tf.placeholder(tf.float32, shape=[None,10])\n",
    "\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([28*28*1,28*28*1]))\n",
    "B1 = tf.Variable(tf.random_normal([28*28*1]))\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+B1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([28*28*1,28*28*1]))\n",
    "B2 = tf.Variable(tf.random_normal([28*28*1]))\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1,W2)+B2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([28*28*1,10]))\n",
    "B3 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "logits = tf.matmul(layer2, W3)+B3\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "###\n",
    "prediction = tf.argmax(hypothesis,1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess = tf.Session()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# for step in range(1000):\n",
    "#     cost_v, _ = sess.run([cost, optimizer], feed_dict={X:x_train,Y:y_train})\n",
    "#     if step %100 ==0 :\n",
    "#         print('step', step, 'cost',cost_v)\n",
    "# print(sess.run(accuracy, feed_dict={X:x_train,Y:y_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epoch = 5\n",
    "batch_size = 100\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(training_epoch):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size) #550\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        c, _ = sess.run([cost, optimizer],feed_dict={X:batch_xs,Y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "    print(\"Epoch :\",epoch, 'Cost', avg_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sess.run(accuracy, feed_dict={X:x_test, Y:y_test})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 튜닝해보기\n",
    "## sigmoid -> relu\n",
    "## gradientdecent -> adamoptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= tf.placeholder(tf.float32, shape=[None,28*28*1])\n",
    "Y= tf.placeholder(tf.float32, shape=[None,10])\n",
    "\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([28*28*1,28*28*1]))\n",
    "B1 = tf.Variable(tf.random_normal([28*28*1]))\n",
    "layer1 = tf.nn.relu(tf.matmul(X,W1)+B1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([28*28*1,28*28*1]))\n",
    "B2 = tf.Variable(tf.random_normal([28*28*1]))\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1,W2)+B2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([28*28*1,10]))\n",
    "B3 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "logits = tf.matmul(layer2, W3)+B3\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "###\n",
    "prediction = tf.argmax(hypothesis,1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epoch = 5\n",
    "batch_size = 100\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(training_epoch):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size) #550\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        c, _ = sess.run([cost, optimizer],feed_dict={X:batch_xs,Y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "    print(\"Epoch :\",epoch, 'Cost', avg_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sess.run(accuracy, feed_dict={X:x_test, Y:y_test})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = random.randint(0,mnist.test.num_examples-1)\n",
    "print('Lbael:', sess.run(tf.argmax(mnist.test.labels[r:r+1],1)))\n",
    "print('Lbael:', sess.run(tf.argmax(hypothesis,1),feed_dict={X:mnist.test.images[r:r+1]}))\n",
    "plt.imshow(\n",
    "mnist.test.images[r:r+1].reshape(28,28),\n",
    "    cmap='Greys',\n",
    "    interpolation = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= tf.placeholder(tf.float32, shape=[None,28*28*1])\n",
    "Y= tf.placeholder(tf.float32, shape=[None,10])\n",
    "\n",
    "W1 = tf.get_variable(\"W1\",shape=[28*28*1,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "B1 = tf.Variable(tf.random_normal([256]))\n",
    "layer1 = tf.nn.relu(tf.matmul(X,W1)+B1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\",shape=[256,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "B2 = tf.Variable(tf.random_normal([256]))\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1,W2)+B2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\",shape=[256,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "B3 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "logits = tf.matmul(layer2, W3)+B3\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "###\n",
    "prediction = tf.argmax(hypothesis,1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epoch = 5\n",
    "batch_size = 100\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(training_epoch):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size) #550\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        c, _ = sess.run([cost, optimizer],feed_dict={X:batch_xs,Y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "    print(\"Epoch :\",epoch, 'Cost', avg_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sess.run(accuracy, feed_dict={X:x_test, Y:y_test})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 오버피팅 방지 방법 drop out 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-608301bbeea3>:22: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-608301bbeea3>:22: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X= tf.placeholder(tf.float32, shape=[None,28*28*1])\n",
    "Y= tf.placeholder(tf.float32, shape=[None,10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.get_variable(\"W1\",shape=[28*28*1,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "B1 = tf.Variable(tf.random_normal([256]))\n",
    "layer1 = tf.nn.relu(tf.matmul(X,W1)+B1)\n",
    "layer1 = tf.nn.dropout(layer1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\",shape=[256,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "B2 = tf.Variable(tf.random_normal([256]))\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1,W2)+B2)\n",
    "layer2 = tf.nn.dropout(layer2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\",shape=[256,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "B3 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "logits = tf.matmul(layer2, W3)+B3\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "###\n",
    "prediction = tf.argmax(hypothesis,1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 Cost 0.41812248323451356\n",
      "Epoch : 1 Cost 0.2936092029918322\n",
      "Epoch : 2 Cost 0.2722450764477253\n",
      "Epoch : 3 Cost 0.28051969552581957\n",
      "Epoch : 4 Cost 0.26473247682167744\n"
     ]
    }
   ],
   "source": [
    "training_epoch = 5\n",
    "batch_size = 100\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(training_epoch):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size) #550\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        c, _ = sess.run([cost, optimizer],feed_dict={X:batch_xs,Y:batch_ys,keep_prob:0.7})\n",
    "        avg_cost += c/total_batch\n",
    "    print(\"Epoch :\",epoch, 'Cost', avg_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9587"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = sess.run(accuracy, feed_dict={X:x_test, Y:y_test, keep_prob : 1})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epoch = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN  Layer\n",
    "\n",
    "# 1층 : 3*3 필터 32개 (dropout rate: 0.3 )\n",
    "# relu-max pooling\n",
    "# 2층 : 3*3 필터 64개 (dropout rate: 0.3 )\n",
    "# relu-max pooling\n",
    "# 3층 : 3*3 필터 128개 (dropout rate: 0.3 )\n",
    "# relu-max pooling\n",
    "\n",
    "# DNN(FC)  Layer\n",
    "# Initializer : xavier initializer\n",
    "# 1층 : hidden node 625개 (dropout rate: 0.3 )\n",
    "# activation function : Relu\n",
    "# 2층 : hidden node 256개 (dropout rate: 0.3 )\n",
    "\n",
    "# Softmax Layer로 multi label classification\n",
    "# AdamOptimizer\n",
    "# learning_rate : 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])   # 첫 번째가 개수, 나머지는 이미지의 가로/세로/높이\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))   # 필터 개수는 자유롭게 지정 가능(크기, 개수), 1시그마를 0.01로 해서 초기화값의 범위를 줄임\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')   # 심층신경망 통과한 L1의 크기 -1*28*28(same)*32(필터 개수) -> -1 = None\n",
    "# SAME = stride가 1인 기준으로 이미지가 똑같이 나온다 -> 2는 이미지가 동일한 크기여야 하므로 커널을 통과한 이미지는 1/2 크기가 된다\n",
    "\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize-[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "# 여기서의 SAME은 zero padding을 하지 않음(padding은 stride가 1일 때를 기준으로 해서 strides가 2면 이미지의 크기가 절반으로 줄어든다\n",
    "# L1 = -1*14*14*32(max pooling)\n",
    "# pooling의 커널과 conventional의 필터는 서로 다른 개념이다 ->\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))   # 64만 변경 가능(3*3은 필터 크기가 같고, 32는 이전 커널을 통과한 개수이므로 변경 불가)\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding=\"SAME\")   # L2 = -1*14*14*64 ->\n",
    "\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')   # L2 = -1 * 7 * 7 * 64\n",
    "\n",
    "# FC에 넣기 위해서는 펼쳐서 넣어야 한다\n",
    "L2_flat = tf.reshape(L2, [-1, 7*7*64])\n",
    "w3 = tf.get_variable(\"W3\", shape=[7*7*64, 10])   # 입력 데이터의 개수 = 7*7*64, 출력 데이터의 개수 = 라벨(0~9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.Variable(tf.random_normal([10]))\n",
    "logits = tf.matmul(L2_flat, W3) + b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epoch):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)  \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)  \n",
    "        c, _ = sess.run([cost, optimizer], feed_dict={X:batch_xs, Y:batch_ys})\n",
    "        avg_cost += c/total_batch \n",
    "    print(\"Epoch: \", epoch, \"Cost: \", avg_cost) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
